{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/injy/mariam_workspace/env3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/injy/mariam_workspace/env3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/injy/mariam_workspace/env3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/injy/mariam_workspace/env3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/injy/mariam_workspace/env3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/injy/mariam_workspace/env3/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../.')\n",
    "import numpy as np\n",
    "import math\n",
    "from data.data_utils.reader_utils import read_calib\n",
    "import tensorflow as tf\n",
    "from data.detection_dataset_loader import *\n",
    "from data.data_utils.data_reader import *\n",
    "from data.data_utils.reader_utils import *\n",
    "from model import *\n",
    "from data.postprocessing.nms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label2(calib_reader, label_path, shift_h, shift_w, x_range=(0, 71), y_range=(-40, 40), z_range=(-3.0, 1), \n",
    "                    size=(512, 448, 40), get_actual_dims=False, from_file=True, translate_x=0, translate_y=0, translate_z=0, ang=0, get_neg=False):\n",
    "\n",
    "    \"\"\"\n",
    "    the file format is as follows: \n",
    "    type, truncated, occluded, alpha, bbox_left, bbox_top, bbox_right, bbox_bottom,\n",
    "    dimensions_height, dimensions_width, dimensions_length, location_x, location_y, location_z,\n",
    "    rotation_y, score) \n",
    "    \"\"\"\n",
    "    if from_file:\n",
    "        lines = []\n",
    "        with open(label_path) as label_file:\n",
    "            lines = label_file.readlines()\n",
    "    else:\n",
    "        lines = label_path.split('\\n')\n",
    "#     print(len(lines))\n",
    "    # filter car class\n",
    "    lines = list(map(lambda x: x.split(), lines))\n",
    "    if len(lines) > 0:\n",
    "        if get_neg:\n",
    "            lines = list(filter(lambda x: len(x) > 0 and ( x[0] not in ['Car', 'Van', 'Truck', 'Tram', 'DontCare']), lines))\n",
    "            if len(lines) > 0:\n",
    "                lines = lines[:1]\n",
    "        else:\n",
    "            lines = list(filter(lambda x: len(x) > 0 and ( x[0] in ['Car', 'Van', 'Truck', 'Tram']), lines))\n",
    "    \n",
    "    def get_parameter(index):\n",
    "        return list(map(lambda x: x[index], lines))\n",
    "    \n",
    "    classes = np.array(get_parameter(0))\n",
    "    dimension_height = np.array(get_parameter(8)).astype(float)\n",
    "    dimension_width = np.array(get_parameter(9)).astype(float)\n",
    "    dimension_length = np.array(get_parameter(10)).astype(float)\n",
    "    # TODO: take shift into consideration - URGENT\n",
    "    location_x = np.array(get_parameter(11)).astype(float)\n",
    "    location_y = np.array(get_parameter(12)).astype(float)\n",
    "    location_z = np.array(get_parameter(13)).astype(float)\n",
    "    angles = np.array(get_parameter(14)).astype(float)\n",
    "    directions = np.array(angles>= 0).astype(float)\n",
    "    \n",
    "    # print(len(classes))\n",
    "    calib_data = calib_reader.read_calib()\n",
    "\n",
    "    locations = np.array([[location_x[i], location_y[i], location_z[i]] for i in range(len(classes))])\n",
    "\n",
    "    if len(locations) > 0 and len(locations[0]) > 0:\n",
    "        locations = project_rect_to_velo(locations, calib_data['R0_rect'].reshape((3, 3)), calib_data['Tr_velo_to_cam'].reshape((3, 4)))\n",
    "#     print(len(locations))\n",
    "    # print(z_range)\n",
    "\n",
    "    indx = []\n",
    "    i = 0\n",
    "    for point in locations:\n",
    "        if (point[0] >= x_range[0]  and point[0] <= x_range[1])\\\n",
    "            and (point[1] >= y_range[0] and point[1] <= y_range[1])\\\n",
    "            and (point[2] >= z_range[0] and point[2] <= z_range[1]):\n",
    "            indx.append(i)\n",
    "        i += 1\n",
    "\n",
    "    indxes = np.array(list(map(lambda point: (point[0] >= x_range[0]  and point[0] <= x_range[1])\n",
    "                                    and (point[1] >= y_range[0] and point[1] <= y_range[1])\n",
    "                                    and (point[2] >= z_range[0] and point[2] <= z_range[1]) , locations)))\n",
    "    locations = np.array(list(filter(lambda point: (point[0] >= x_range[0]  and point[0] <= x_range[1])\n",
    "                                    and (point[1] >= y_range[0] and point[1] <= y_range[1])\n",
    "                                    and (point[2] >= z_range[0] and point[2] <= z_range[1]) , locations)))\n",
    "\n",
    "    if len(indx) > 0:\n",
    "        dimension_height = dimension_height[indx]\n",
    "        dimension_width = dimension_width[indx]\n",
    "        dimension_length = dimension_length[indx]\n",
    "        location_x = location_x[indx]\n",
    "        location_y = location_y[indx]\n",
    "        location_z = location_z[indx]\n",
    "        angles = angles[indx]\n",
    "        classes = classes[indx]\n",
    "        directions = directions[indx]\n",
    "\n",
    "    if len(locations) > 0:\n",
    "        locations[:, :3] = locations[:, :3] - np.array([translate_x, translate_y, -translate_z])\n",
    "\n",
    "    # print('.......')\n",
    "    # print(len(locations))\n",
    "\n",
    "    points = [project_point_from_camera_coor_to_velo_coor([location_x[i], location_y[i], location_z[i]], \n",
    "                                                        [dimension_height[i], dimension_width[i], dimension_length[i]],\n",
    "                                                        angles[i],\n",
    "                                                         calib_data)\n",
    "                for i in range(len(locations))]\n",
    "    \n",
    "    x_size = (x_range[1] - x_range[0])\n",
    "    y_size = (y_range[1] - y_range[0])\n",
    "    z_size = (z_range[1] - z_range[0])\n",
    "            \n",
    "    x_fac = (size[0]-1) / x_size\n",
    "    y_fac = (size[1]-1) / y_size\n",
    "    z_fac = (size[2]-1) / z_size\n",
    "    if get_actual_dims:\n",
    "        import math\n",
    "        for i in range(len(points)):\n",
    "            b = points[i]\n",
    "            x0 = b[0][0]\n",
    "            y0 = b[0][1]\n",
    "            x1 = b[1][0]\n",
    "            y1 = b[1][1]\n",
    "            x2 = b[2][0]\n",
    "            y2 = b[2][1]\n",
    "            u0 = -(x0) * x_fac + size[0]\n",
    "            v0 = -(y0 + 40) * y_fac + size[1]\n",
    "            u1 = -(x1) * x_fac + size[0]\n",
    "            v1 = -(y1 + 40) * y_fac + size[1]\n",
    "            u2 = -(x2) * x_fac + size[0]\n",
    "            v2 = -(y2 + 40) * y_fac + size[1]\n",
    "            dimension_length[i] = math.sqrt((v1-v2)**2 + (u1-u2)**2)\n",
    "            dimension_width[i] = math.sqrt((v1-v0)**2 + (u1-u0)**2)\n",
    "            dimension_height[i] = math.sqrt((-(b[0][2]+(-1*z_range[1]))*z_fac-(-b[4][2]+z_range[1])*z_fac)**2)\n",
    "\n",
    "      \n",
    "    for i in range(len(locations)):\n",
    "        if angles[i] < 0:\n",
    "            angles[i] += 3.14\n",
    "\n",
    "    x_range = (x_range[0] + translate_x, x_range[1] + translate_x)\n",
    "    y_range = (y_range[0] + translate_y, y_range[1] + translate_y)\n",
    "    z_range = (z_range[0] + translate_z, z_range[1] + translate_z)\n",
    "    output = [[-(locations[i][0] + -1*x_range[0]) * x_fac + size[0], -(locations[i][1] + -1*y_range[0]) * y_fac + size[1], -(locations[i][2] + -1*z_range[0]) * z_fac + size[2], \n",
    "                dimension_length[i], dimension_width[i], dimension_height[i], angles[i]] \n",
    "                for i in range(len(locations))]\n",
    "    # import math\n",
    "    if ang != 0:\n",
    "        for i in range(len(locations)):\n",
    "            w = size[0]\n",
    "            h = size[1]\n",
    "            output[i][0], output[i][1] = rotate2((w//2, h//2), (output[i][0], output[i][1]), ang / 57.2958)\n",
    "            output[i][6] = output[i][6] - ang / 57.2958\n",
    "\n",
    "    output = list(filter(lambda point: 0 <= point[0] < size[0] and 0 <= point[1] < size[1] and 0 <= point[2] < size[2] , output))\n",
    "    output = np.array(output)\n",
    "\n",
    "    if from_file:\n",
    "        return points, output, calib_data['Tr_velo_to_cam'], calib_data['R0_rect'], calib_data['P2'], directions\n",
    "    else:\n",
    "        return output, indxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms2(label, scores, max_output_size=100, iou_threshold=0.1, sess=None):\n",
    "    boxes = []\n",
    "\n",
    "    for j in range(0, len(label)):\n",
    "\n",
    "        w = label[j][3]\n",
    "        h = label[j][4] \n",
    "        x = label[j][0]\n",
    "        y = label[j][1]\n",
    "        a = label[j][6]\n",
    "        \n",
    "\n",
    "        polygon = convert5Pointto8Point(y, x, w, h, -a*57.2958)\n",
    "        xs = polygon[0::2]\n",
    "        ys = polygon[1::2]\n",
    "            \n",
    "        boxes.append([xs[0], ys[0], xs[2], ys[2]])\n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "    with tf.Graph().as_default():\n",
    "        selected_indices = tf.image.non_max_suppression(\n",
    "                    boxes, scores, max_output_size=max_output_size, iou_threshold=iou_threshold)\n",
    "        if sess is not None:\n",
    "            selected_indices = sess.run(selected_indices)\n",
    "        else:\n",
    "            with tf.Session() as sess:\n",
    "                selected_indices = sess.run(selected_indices)\n",
    "\n",
    "    return selected_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cart2hom(pts_3d):\n",
    "    ''' Input: nx3 points in Cartesian\n",
    "        Oupput: nx4 points in Homogeneous by pending 1\n",
    "    '''\n",
    "    n = pts_3d.shape[0]\n",
    "    pts_3d_hom = np.hstack((pts_3d, np.ones((n,1))))\n",
    "    return pts_3d_hom\n",
    "\n",
    "def project_velo_to_ref(pts_3d_velo, Tr_velo_to_cam):\n",
    "    pts_3d_velo = cart2hom(pts_3d_velo) # nx4\n",
    "    return np.dot(pts_3d_velo, np.transpose(Tr_velo_to_cam))\n",
    "\n",
    "    \n",
    "def project_ref_to_rect(pts_3d_ref, R0_rect):\n",
    "        ''' Input and Output are nx3 points '''\n",
    "        return np.transpose(np.dot(R0_rect, np.transpose(pts_3d_ref)))\n",
    "\n",
    "def ProjectTo2Dbbox(center, h, w, l, r_y, P2):\n",
    "    # input: 3Dbbox in (rectified) camera coords\n",
    "\n",
    "    Rmat = np.asarray([[math.cos(r_y), 0, math.sin(r_y)],\n",
    "                       [0, 1, 0],\n",
    "                       [-math.sin(r_y), 0, math.cos(r_y)]],\n",
    "                       dtype='float32')\n",
    "\n",
    "    p0 = center + np.dot(Rmat, np.asarray([l/2.0, 0, w/2.0], dtype='float32').flatten())\n",
    "    p1 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, w/2.0], dtype='float32').flatten())\n",
    "    p2 = center + np.dot(Rmat, np.asarray([-l/2.0, 0, -w/2.0], dtype='float32').flatten())\n",
    "    p3 = center + np.dot(Rmat, np.asarray([l/2.0, 0, -w/2.0], dtype='float32').flatten())\n",
    "    p4 = center + np.dot(Rmat, np.asarray([l/2.0, -h, w/2.0], dtype='float32').flatten())\n",
    "    p5 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, w/2.0], dtype='float32').flatten())\n",
    "    p6 = center + np.dot(Rmat, np.asarray([-l/2.0, -h, -w/2.0], dtype='float32').flatten())\n",
    "    p7 = center + np.dot(Rmat, np.asarray([l/2.0, -h, -w/2.0], dtype='float32').flatten())\n",
    "\n",
    "    points = np.array([p0, p1, p2, p3, p4, p5, p6, p7])\n",
    "\n",
    "    points_hom = np.ones((points.shape[0], 4)) # (shape: (8, 4))\n",
    "    points_hom[:, 0:3] = points\n",
    "\n",
    "    # project the points onto the image plane (homogeneous coords):\n",
    "    img_points_hom = np.dot(P2, points_hom.T).T # (shape: (8, 3)) (points_hom.T has shape (4, 8))\n",
    "    # normalize:\n",
    "    img_points = np.zeros((img_points_hom.shape[0], 2)) # (shape: (8, 2))\n",
    "    img_points[:, 0] = img_points_hom[:, 0]/img_points_hom[:, 2]\n",
    "    img_points[:, 1] = img_points_hom[:, 1]/img_points_hom[:, 2]\n",
    "\n",
    "    u_min = np.min(img_points[:, 0])\n",
    "    v_min = np.min(img_points[:, 1])\n",
    "    u_max = np.max(img_points[:, 0])\n",
    "    v_max = np.max(img_points[:, 1])\n",
    "\n",
    "    left = int(u_min)\n",
    "    top = int(v_min)\n",
    "    right = int(u_max)\n",
    "    bottom = int(v_max)\n",
    "\n",
    "    projected_2Dbbox = [left, top, right, bottom]\n",
    "\n",
    "    return projected_2Dbbox\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = x.astype(np.float128)\n",
    "    x = 1 / (1 + np.exp(-x))\n",
    "    x = x.astype(np.float32)\n",
    "    return x\n",
    "\n",
    "def convert_prediction_into_real_values(label_tensor, truth_value=None,\n",
    "            anchors=np.array([3.9, 1.6, 1.5]), \n",
    "            input_size=(512, 448), output_size=(128, 112), is_label=False, th=0.5):\n",
    "\n",
    "    ratio = input_size[0] // output_size[0]\n",
    "    result = []\n",
    "    if not is_label:\n",
    "        ones_index = np.where(sigmoid(label_tensor[:, :, :, -1])>=th)\n",
    "    else:\n",
    "        ones_index = np.where(label_tensor[:, :, :, -1]>=th)\n",
    "    if truth_value is not None:\n",
    "        ones_index = np.where(truth_value[:, :, :, -1]>=th)\n",
    "#     print(len(ones_index))\n",
    "    if len(ones_index) > 0 and len(ones_index[0]) > 0:\n",
    "        for i in range(0, len(ones_index[0]), 1):\n",
    "            x = ones_index[0][i]\n",
    "            y = ones_index[1][i]\n",
    "            \n",
    "            out = np.copy(label_tensor[ones_index[0][i], ones_index[1][i], ones_index[2][i], :])\n",
    "            anchor = np.array([x+0.5, y+0.5, 1., anchors[0], anchors[1], anchors[2]])\n",
    "\n",
    "            out[:3] = out[:3] * anchor[3:6] + anchor[:3]\n",
    "            \n",
    "            out[:2] = out[:2] * ratio\n",
    "            out[2] = out[2] * 40\n",
    "            \n",
    "            out[3:6] = np.exp(out[3:6]) * anchors\n",
    "            \n",
    "            k = ones_index[2][i]\n",
    "            if not is_label:\n",
    "              out[6] = sigmoid(out[6]) * np.pi/2 - np.pi/4\n",
    "            else:\n",
    "                out[6] = out[6]\n",
    "            \n",
    "            if k == 0 and out[6] < 0:\n",
    "                out[6] = out[6] + np.pi\n",
    "                \n",
    "            out[6] = out[6] + k * (np.pi/2)\n",
    "                        \n",
    "            result.append(out)\n",
    "            \n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "def get_points(converted_points, calib_path, \n",
    "                x_range=(0, 71), y_range=(-40, 40), z_range=(-3.0, 1), \n",
    "                size=(512, 448, 40), th=0.5):\n",
    "    all_result = []\n",
    "    for converted_points_ in converted_points:\n",
    "        if sigmoid(converted_points_[-1]) >= th:\n",
    "            result = [0] * 16\n",
    "            result[0] = 'Car'\n",
    "            result[1] = -1\n",
    "            result[2] = -1\n",
    "            result[3] = -10\n",
    "            result[8] = converted_points_[5]\n",
    "            result[9] = converted_points_[4]\n",
    "            result[10] = converted_points_[3]\n",
    "            result[14] = converted_points_[6]\n",
    "            result[15] = sigmoid(converted_points_[-1])\n",
    "\n",
    "            \n",
    "            calib_reader = CalibReader(calib_path)\n",
    "            calib_data = calib_reader.read_calib()\n",
    "\n",
    "            x_size = (x_range[1] - x_range[0])\n",
    "            y_size = (y_range[1] - y_range[0])\n",
    "            z_size = (z_range[1] - z_range[0])\n",
    "\n",
    "            x_fac = (size[0]-1) / x_size\n",
    "            y_fac = (size[1]-1) / y_size\n",
    "            z_fac = (size[2]-1) / z_size\n",
    "\n",
    "            x, y, z = -((converted_points_[:3] - size) / np.array([x_fac, y_fac, z_fac])) - np.array([0, -1*y_range[0], -1*z_range[0]]) \n",
    "            point = np.array([[x, y, z]])\n",
    "            box3d_pts_3d = point\n",
    "\n",
    "            pts_3d_ref = project_velo_to_ref(box3d_pts_3d, calib_data['Tr_velo_to_cam'].reshape((3, 4)))\n",
    "            pts_3d_ref = project_ref_to_rect(pts_3d_ref, calib_data['R0_rect'].reshape((3, 3)))[0]\n",
    "            for k in range(3):\n",
    "                result[11 + k] = pts_3d_ref[k]\n",
    "\n",
    "            imgbbox = ProjectTo2Dbbox(pts_3d_ref, converted_points_[5], converted_points_[4],\n",
    "                         converted_points_[3], converted_points_[6], calib_data['P2'].reshape((3, 4)))\n",
    "\n",
    "            result[4:8] = imgbbox\n",
    "            all_result.append(result)\n",
    "    return all_result\n",
    "\n",
    "\n",
    "def prepare_dataset_feed_dict(model, dataset, train_fusion_rgb):\n",
    "        data = dataset.get_next(batch_size=1)\n",
    "        camera_tensor, lidar_tensor, label_tensor= data\n",
    "        d = {model.train_inputs_rgb: camera_tensor,\n",
    "                model.train_inputs_lidar: lidar_tensor,\n",
    "                model.y_true: label_tensor,\n",
    "                model.train_fusion_rgb: train_fusion_rgb,\n",
    "                model.is_training: False,\n",
    "                model.weight_cls: 1,\n",
    "                model.weight_dim: 1,\n",
    "                model.weight_loc: 1,\n",
    "                model.weight_theta: 1}\n",
    "        return d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def write_predictions(labels_output, calib_path, new_file_path, th=0.5, truth_value=None, is_label=False, base_path = '../../../Data'):\n",
    "    converted_points = convert_prediction_into_real_values(labels_output, truth_value=truth_value, th=th, is_label=is_label)\n",
    "#     print(len(converted_points))\n",
    "    points = get_points(converted_points, calib_path, th=th)\n",
    "#     print(len(points))\n",
    "#     print('---')\n",
    "    res = '\\n'.join([' '.join([str(l) for l in points[i]]) for i in range(len(points))])\n",
    "    text_file = open(new_file_path, \"wb+\")\n",
    "    text_file.write(res.encode())\n",
    "    text_file.close()\n",
    "\n",
    "def write_all_predictions(model, dir_name, training, augment=False, get_best=False, fusion=False, base_path = '../../../Data'):\n",
    "    with model.graph.as_default():\n",
    "            \n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "\n",
    "        with tf.Session(config=config) as sess:\n",
    "            if get_best:\n",
    "                model.saver.restore(sess, tf.train.latest_checkpoint('../training_files/tmp_best2/'))\n",
    "            else:\n",
    "                model.saver.restore(sess, tf.train.latest_checkpoint('../training_files/tmp/'))\n",
    "\n",
    "            dataset = DetectionDatasetLoader(base_path='../../../Data', training_per=0.5, batch_size=1, random_seed=0, training=training, augment=augment)\n",
    "        \n",
    "            cls_losses = []\n",
    "            reg_losses = []\n",
    "            total_losses = []\n",
    "            i = 0\n",
    "            \n",
    "            apply_nms=False\n",
    "\n",
    "            if training:\n",
    "                file_name = '/trainsplit.txt'\n",
    "            else:\n",
    "                file_name = '/valsplit.txt'\n",
    "            with open(base_path + file_name, 'r') as f:\n",
    "                            list_file_nums = f.readlines()\n",
    "            list_files = ['0'*(6-len(l.strip())) + l.strip() for l in list_file_nums]\n",
    "            list_calib_paths = list(map(lambda x: base_path + '/data_object_calib/training/calib/' + x + '.txt', list_files))\n",
    "            \n",
    "            try:    \n",
    "                while True:\n",
    "                    feed_dict = prepare_dataset_feed_dict(model, dataset, fusion)\n",
    "                    final_output= sess.run(model.final_output, feed_dict=feed_dict)\n",
    "#                     print(i, np.where(sigmoid(final_output[0][:, :, :, -1])>=0.2))\n",
    "                    if i < len(list_files):\n",
    "                        current_file = list_files[i]\n",
    "\n",
    "                        for th, th_str in zip([0.05, 0.1, 0.2, 0.3, 0.4, 0.5], ['05', '10', '20', '30', '40', '50']):\n",
    "                            new_file_path = '../prediction_files/' + dir_name + '/bev/th' + th_str + '_2/data/' + current_file + '.txt'\n",
    "                            write_predictions(final_output[0], list_calib_paths[i], new_file_path, th=th)\n",
    "                            \n",
    "                    else:\n",
    "                        break\n",
    "\n",
    "                    i += 1\n",
    "                    if i % 100 == 0:\n",
    "                        print('i = ', i)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                pass\n",
    "            except StopIteration:\n",
    "                pass\n",
    "            finally:\n",
    "                print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions_nms(th, new_file_path, old_file_path, calib_reader, base_path = '../../../Data'):\n",
    "    \n",
    "    res = []\n",
    "    with open(old_file_path) as label_file:\n",
    "            res = label_file.readlines()\n",
    "    res = ''.join(res)\n",
    "    points2 = res.split('\\n')\n",
    "    points = []\n",
    "    for i in range(len(points2)):\n",
    "        temp = []\n",
    "        temp = points2[i].split(' ')\n",
    "        points.append(temp)\n",
    "#     print('here')\n",
    "    labels, indxes = read_label2(calib_reader, res, 0, 0, get_actual_dims=True, from_file=False)\n",
    "\n",
    "    points = np.array(points)\n",
    "#     print('here')\n",
    "    if len(labels) > 0:\n",
    "        points = points[indxes]\n",
    "        selected_idx = nms2(labels, np.array([points[i][-1] for i in range(len(points))]), max_output_size=100, iou_threshold=0.3)\n",
    "    else:\n",
    "        selected_idx = []\n",
    "\n",
    "    if len(selected_idx) > 0:\n",
    "        points = points[selected_idx]\n",
    "        res = '\\n'.join([' '.join([str(l) for l in points[i]]) for i in range(len(points))])\n",
    "    else:\n",
    "        res=\"\"\n",
    "    \n",
    "    text_file = open(new_file_path, \"wb+\")\n",
    "    text_file.write(res.encode())\n",
    "    text_file.close()\n",
    "\n",
    "def get_augmentation_parameters():\n",
    "                    image_translate_x = 0\n",
    "                    image_translate_y = 0\n",
    "\n",
    "                    translate_x = 0\n",
    "                    translate_y = 0\n",
    "                    translate_z = 0\n",
    "                    ang = 0\n",
    "\n",
    "                    r = R.from_rotvec(np.radians(0) * np.array([0, 0, 1]))\n",
    "                    rot = r.as_dcm()\n",
    "                    rot = np.append(rot, np.array([[0,0,0]]), axis=0)\n",
    "                    rot = np.append(rot, np.array([[0],[0],[0],[1]]), axis=1)\n",
    "\n",
    "                    tr_x = 0\n",
    "                    tr_y = 0\n",
    "                    tr_z = 0\n",
    "                    tr = np.array([[tr_x], [tr_y], [tr_z], [0]])\n",
    "\n",
    "                    sc_x = 1\n",
    "                    sc_y = 1\n",
    "                    sc_z = 1\n",
    "                    sc = np.array([[sc_x, 0, 0, 0], [0, sc_y, 0, 0], [0, 0, sc_z, 0], [0, 0, 0, 1]])\n",
    "\n",
    "                    return rot, tr, sc, image_translate_x, image_translate_y, ang\n",
    "    \n",
    "def write_all_predictions_nms(old_dir, training, base_path = '../../../Data'):\n",
    "    i = 0       \n",
    "    try:    \n",
    "        while True:\n",
    "            \n",
    "            if training:\n",
    "                file_name = '/trainsplit.txt'\n",
    "            else:\n",
    "                            file_name = '/valsplit.txt'\n",
    "            with open(base_path + file_name, 'r') as f:\n",
    "                            list_file_nums = f.readlines()\n",
    "            list_files = ['0'*(6-len(l.strip())) + l.strip() for l in list_file_nums]\n",
    "\n",
    "            list_camera_paths = list(map(lambda x: base_path+'/data_object_image_3/training/image_3/' + x + '.png', list_files))\n",
    "            list_lidar_paths = list(map(lambda x: base_path+'/data_object_velodyne/training/velodyne/' + x + '.bin', list_files))\n",
    "            list_label_paths = list(map(lambda x: base_path + '/data_object_label_2/training/label_2/' + x + '.txt', list_files))\n",
    "            list_calib_paths = list(map(lambda x: base_path + '/data_object_calib/training/calib/' + x + '.txt', list_files))\n",
    "            \n",
    "            rot, tr, sc, image_translate_x, image_translate_y, ang = get_augmentation_parameters()\n",
    "            \n",
    "            if i < len(list_files):\n",
    "                current_file = list_files[i]\n",
    "                data_reader_obj = DataReader(list_camera_paths[i],\n",
    "                             list_calib_paths[i], \n",
    "                             list_label_paths[i], \n",
    "                             list_lidar_paths[i], \n",
    "                            rot, sc, tr, ang, image_translate_x, image_translate_y, get_actual_dims=True)\n",
    "            \n",
    "                for th, th_str in zip([0.05, 0.1, 0.2, 0.3, 0.4, 0.5], ['05', '10', '20', '30', '40', '50']):\n",
    "                    old_file_path = '../prediction_files/' + old_dir + '/bev/th' + th_str + '_2/data/' + current_file + '.txt'\n",
    "                    new_file_path = '../prediction_files/' + old_dir + '/nms/th' + th_str + '_2/data/' + current_file + '.txt'\n",
    "                    write_predictions_nms(th, new_file_path, old_file_path, data_reader_obj.calib_reader)\n",
    "      \n",
    "            else:\n",
    "                break\n",
    "            i += 1\n",
    "            if i % 100 == 0:\n",
    "                print('i = ', i)\n",
    "    except tf.errors.OutOfRangeError as e:\n",
    "        print(\"i at break1 = \", i)\n",
    "    except StopIteration as e:\n",
    "        print(\"i at break2 = \", i)\n",
    "    except Exception as e:\n",
    "        print(\"i at break3 = \", i, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = 'aug_train_2_best'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -r \"../prediction_files/aug_train_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir \"../prediction_files\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th05_2\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th05_2/data\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th10_2\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th10_2/data\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th20_2\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th20_2/data\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th30_2\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th30_2/data\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th40_2\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th40_2/data\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th50_2\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/bev/th50_2/data\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th05_2\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th05_2/data\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th10_2\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th10_2/data\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th20_2\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th20_2/data\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th30_2\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th30_2/data\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th40_2\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th40_2/data\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th50_2\"\n",
    "!mkdir \"../prediction_files/\"\"$dir_name\"\"/nms/th50_2/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/injy/mariam_workspace/env3/lib/python3.5/site-packages/torch/hub.py:411: UserWarning: TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead\n",
      "  warnings.warn('TORCH_MODEL_ZOO is deprecated, please use env TORCH_HOME instead')\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'fusion': False\n",
    "}\n",
    "model = Model(graph=None, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../training_files/tmp_best2/model.ckpt-51548\n",
      "i =  100\n",
      "i =  200\n",
      "i =  300\n",
      "i =  400\n",
      "i =  500\n",
      "i =  600\n",
      "i =  700\n",
      "i =  800\n",
      "i =  900\n",
      "i =  1000\n",
      "i =  1100\n",
      "i =  1200\n",
      "i =  1300\n",
      "i =  1400\n",
      "i =  1500\n",
      "i =  1600\n",
      "i =  1700\n",
      "i =  1800\n",
      "i =  1900\n",
      "i =  2000\n",
      "i =  2100\n",
      "i =  2200\n",
      "i =  2300\n",
      "i =  2400\n",
      "i =  2500\n",
      "i =  2600\n",
      "i =  2700\n",
      "i =  2800\n",
      "i =  2900\n",
      "i =  3000\n",
      "i =  3100\n",
      "i =  3200\n",
      "i =  3300\n",
      "i =  3400\n",
      "i =  3500\n",
      "i =  3600\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "write_all_predictions(model, dir_name, training=True, augment=False, get_best=True, fusion=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../training_files/tmp_best2/model.ckpt-51548\n",
      "i =  100\n",
      "i =  200\n",
      "i =  300\n",
      "i =  400\n",
      "i =  500\n",
      "i =  600\n",
      "i =  700\n",
      "i =  800\n",
      "i =  900\n",
      "i =  1000\n",
      "i =  1100\n",
      "i =  1200\n",
      "i =  1300\n",
      "i =  1400\n",
      "i =  1500\n",
      "i =  1600\n",
      "i =  1700\n",
      "i =  1800\n",
      "i =  1900\n",
      "i =  2000\n",
      "i =  2100\n",
      "i =  2200\n",
      "i =  2300\n",
      "i =  2400\n",
      "i =  2500\n",
      "i =  2600\n",
      "i =  2700\n",
      "i =  2800\n",
      "i =  2900\n",
      "i =  3000\n",
      "i =  3100\n",
      "i =  3200\n",
      "i =  3300\n",
      "i =  3400\n",
      "i =  3500\n",
      "i =  3600\n",
      "i =  3700\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "write_all_predictions(model, dir_name, training=False, augment=False, get_best=True, fusion=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  100\n",
      "i =  200\n",
      "i =  300\n",
      "i =  400\n",
      "i =  500\n",
      "i =  600\n",
      "i =  700\n",
      "i =  800\n",
      "i =  900\n",
      "i =  1000\n",
      "i =  1100\n",
      "i =  1200\n",
      "i =  1300\n",
      "i =  1400\n",
      "i =  1500\n",
      "i =  1600\n",
      "i =  1700\n",
      "i =  1800\n",
      "i =  1900\n",
      "i =  2000\n",
      "i =  2100\n",
      "i =  2200\n",
      "i =  2300\n",
      "i =  2400\n",
      "i =  2500\n",
      "i =  2600\n",
      "i =  2700\n",
      "i =  2800\n",
      "i =  2900\n",
      "i =  3000\n",
      "i =  3100\n",
      "i =  3200\n",
      "i =  3300\n",
      "i =  3400\n",
      "i =  3500\n",
      "i =  3600\n",
      "i =  3700\n"
     ]
    }
   ],
   "source": [
    "write_all_predictions_nms(dir_name, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i =  100\n",
      "i =  200\n",
      "i =  300\n",
      "i =  400\n",
      "i =  500\n",
      "i =  600\n",
      "i =  700\n",
      "i =  800\n",
      "i =  900\n",
      "i =  1000\n",
      "i =  1100\n",
      "i =  1200\n",
      "i =  1300\n",
      "i =  1400\n",
      "i =  1500\n",
      "i =  1600\n",
      "i =  1700\n",
      "i =  1800\n",
      "i =  1900\n",
      "i =  2000\n",
      "i =  2100\n",
      "i =  2200\n",
      "i =  2300\n",
      "i =  2400\n",
      "i =  2500\n",
      "i =  2600\n",
      "i =  2700\n",
      "i =  2800\n",
      "i =  2900\n",
      "i =  3000\n",
      "i =  3100\n",
      "i =  3200\n",
      "i =  3300\n",
      "i =  3400\n",
      "i =  3500\n",
      "i =  3600\n"
     ]
    }
   ],
   "source": [
    "write_all_predictions_nms(dir_name, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3",
   "language": "python",
   "name": "env3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
